{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1c4096df",
   "metadata": {},
   "source": [
    "\n",
    "# Testing a Dummy LLM Chain with LangSmith\n",
    "\n",
    "This notebook demonstrates how to evaluate a simple language‑model chain using [LangSmith](https://smith.langchain.com/).  LangSmith provides tools for debugging, evaluation, performance monitoring and observability so you can build reliable LLM applications[1][1].  Here we use a dummy chain to keep the example fully self‑contained; you can replace it with any LangChain chain or model.\n",
    "\n",
    "We'll create a small dataset of questions and expected answers, run the dummy chain, compute a simple accuracy metric and (optionally) upload the results to LangSmith.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dbafadf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "\n",
    "# If you want to upload results to LangSmith, install the package\n",
    "# and set the LANGSMITH_API_KEY environment variable.\n",
    "try:\n",
    "    from langsmith import Client\n",
    "except ImportError:\n",
    "    Client = None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48748d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a list of examples (question, expected answer)\n",
    "examples = [\n",
    "    {\"question\": \"What is the capital of France?\", \"expected\": \"Paris\"},\n",
    "    {\"question\": \"Who wrote 'Moby Dick'?\", \"expected\": \"Herman Melville\"},\n",
    "    {\"question\": \"2 + 2 = ?\", \"expected\": \"4\"},\n",
    "    {\"question\": \"What year did the first man land on the moon?\", \"expected\": \"1969\"},\n",
    "]\n",
    "\n",
    "pd.DataFrame(examples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f17b51d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define a dummy chain that returns hard‑coded answers\n",
    "\n",
    "def dummy_chain(question: str) -> str:\n",
    "    if \"capital of France\" in question.lower():\n",
    "        return \"Paris\"\n",
    "    if \"moby dick\" in question.lower():\n",
    "        return \"Herman Melville\"\n",
    "    if \"2 + 2\" in question or \"2+2\" in question:\n",
    "        return \"4\"\n",
    "    if \"first man land on the moon\" in question.lower():\n",
    "        return \"1969\"\n",
    "    return \"I don't know\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96561c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Evaluate the dummy chain on each example\n",
    "results = []\n",
    "for ex in examples:\n",
    "    q = ex[\"question\"]\n",
    "    expected = ex[\"expected\"]\n",
    "    predicted = dummy_chain(q)\n",
    "    results.append({\n",
    "        \"question\": q,\n",
    "        \"expected\": expected,\n",
    "        \"predicted\": predicted,\n",
    "        \"correct\": predicted.strip().lower() == expected.strip().lower(),\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "accuracy = results_df[\"correct\"].mean()\n",
    "\n",
    "results_df, accuracy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4daf8a",
   "metadata": {},
   "source": [
    "### Statistical evaluation\n",
    "\n",
    "In addition to counting correct answers, we compute statistical metrics such as accuracy,\n",
    "precision, recall and F1‑score. These metrics help us understand not only how often the model is correct,\n",
    "but also how it balances false positives and false negatives. A confusion matrix summarises predictions\n",
    "across classes and is a useful diagnostic when working with multi‑class outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de467b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute statistical metrics\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "labels = [r['expected'] for r in results]\n",
    "preds = [r['predicted'] for r in results]\n",
    "\n",
    "# Overall accuracy\n",
    "acc = accuracy_score(labels, preds)\n",
    "# Precision, recall and F1 (macro‑averaged)\n",
    "precision, recall, f1, _ = precision_recall_fscore_support(labels, preds, average='macro', zero_division=0)\n",
    "# Confusion matrix as a DataFrame\n",
    "cm = pd.DataFrame(confusion_matrix(labels, preds),\n",
    "                   index=sorted(set(labels)),\n",
    "                   columns=sorted(set(labels)))\n",
    "\n",
    "print(f'Accuracy: {acc:.2f}')\n",
    "print(f'Precision (macro): {precision:.2f}')\n",
    "print(f'Recall (macro): {recall:.2f}')\n",
    "print(f'F1‑score (macro): {f1:.2f}')\n",
    "print('Confusion matrix:')\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a59759",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (Optional) Upload the dataset and results to LangSmith\n",
    "# Requires an API key set in LANGSMITH_API_KEY.\n",
    "import os\n",
    "if Client and os.getenv(\"LANGSMITH_API_KEY\"):\n",
    "    client = Client()\n",
    "    dataset = client.get_or_create_dataset(name=\"demo_dataset_notebook\", description=\"Demo dataset from notebook\")\n",
    "    # Prepare examples\n",
    "    smith_examples = []\n",
    "    for row in results:\n",
    "        smith_examples.append({\n",
    "            \"inputs\": {\"question\": row[\"question\"]},\n",
    "            \"outputs\": {\"answer\": row[\"predicted\"]},\n",
    "            \"expected\": row[\"expected\"],\n",
    "        })\n",
    "    client.create_examples(dataset_id=dataset.id, examples=smith_examples)\n",
    "    print(f\"Uploaded {len(smith_examples)} examples to LangSmith.\")\n",
    "else:\n",
    "    print(\"Set LANGSMITH_API_KEY to upload results.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439e45e1",
   "metadata": {},
   "source": [
    "\n",
    "In this notebook we built a miniature evaluation pipeline for a language‑model chain.  We created a dataset of question/answer pairs, defined a dummy chain that returns fixed answers, computed an accuracy metric and demonstrated how to upload the results to LangSmith.  In a real project you would replace the dummy chain with your own LangChain chain or call to an LLM, and you could use LangSmith's built‑in evaluators to measure metrics such as relevance, exact match or embedding similarity[1].\n",
    "\n",
    "LangSmith's unified dashboard allows you to trace prompts and responses, debug errors and monitor performance in real time[1].  Continuous evaluation and monitoring make it easier to keep your LLM application reliable as it grows and changes[1].\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
